{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced07a5d-dc7a-452f-8897-88479301c5fe",
   "metadata": {
    "id": "ced07a5d-dc7a-452f-8897-88479301c5fe"
   },
   "outputs": [],
   "source": [
    "######################################### IMPORTING PACAKGES #############################\n",
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys  \n",
    "import os\n",
    "from dateutil.parser import parse\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "# PDF text extraction\n",
    "from pdfminer3.layout import LAParams, LTTextBox\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "from pdfminer3.pdfinterp import PDFResourceManager\n",
    "from pdfminer3.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer3.converter import PDFPageAggregator\n",
    "from pdfminer3.converter import TextConverter\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "# Others\n",
    "import requests\n",
    "import string\n",
    "import re\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "import io\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import spacy\n",
    "# spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner'])\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from sklearn.feature_extraction import text\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(stop_words)\n",
    "import pickle #pickle模块\n",
    "import tensorflow as tf\n",
    "from utils import extract_pdf,extract_pages_sentences,keyword_filter,custom_standardization,removeStopWords,stemSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca9bc7f-8cf1-40af-a8ef-0b2a2e8da9ac",
   "metadata": {
    "id": "7ca9bc7f-8cf1-40af-a8ef-0b2a2e8da9ac"
   },
   "outputs": [],
   "source": [
    "def predict(pdf_file,model,vectorizer):\n",
    "    text = extract_pdf(pdf_file)\n",
    "    pages_content, pages_sentences = extract_pages_sentences(spacy.load(\"en_core_web_sm\"),text)\n",
    "    df = keyword_filter(pages_sentences,['biodiversity','green space','program','animal','fish','bird','avian','tree','forest','coastal','beach','shoreline',\n",
    "                                         'clean-up','specie','ecosystem','system','project','protection','conservation','natural resources','wildlife','habitat'])\n",
    "    df = df.reset_index()\n",
    "    df['sentence1'] = df['sentence'].apply(lambda x:tf.compat.as_str_any(custom_standardization(x).numpy()))\n",
    "    df['sentence1'] = df['sentence1'].apply(lambda x:removeStopWords(str(x)))\n",
    "    df['sentence1'] = df['sentence1'].apply(lambda x:stemSentence(str(x)))\n",
    "    X = vectorizer.transform(df['sentence'])\n",
    "    res = model.predict(X)\n",
    "    sentences = list(df[res == 1]['sentence'])\n",
    "    if len(sentences)>0:\n",
    "        label = 'Yes'\n",
    "    else:\n",
    "        label = 'False'\n",
    "    return label, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ea36113-0087-4a3a-8a27-a551fae2f3ca",
   "metadata": {
    "id": "2ea36113-0087-4a3a-8a27-a551fae2f3ca"
   },
   "outputs": [],
   "source": [
    "file = 'ubm_esg_report_2021.pdf'\n",
    "with open('A8_model_randomforest.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "with open('vectorizer_8.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load( f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c15b924-2d21-43e1-b72f-247f936ed012",
   "metadata": {
    "id": "0c15b924-2d21-43e1-b72f-247f936ed012",
    "outputId": "a1074cb9-e2a7-4886-d247-79f4c837f5c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Yes',\n",
       " ['Environmental risks Environmental risks and their impact are becoming increas-ingly important for the planning and realisation of develop-ment projects.',\n",
       "  'We also exchanged an area of 24,290 m on four brownfield projects.',\n",
       "  'For us, that means designing projects to minimise the poten-tial negative effects on the local area, for example caused by shading, artificial light, noise, emission and increased traffic, or to more than offset these effects through positive changes and improvements.'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(file,model,vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7eec1f-873a-4c71-97f2-ee3492dd14f9",
   "metadata": {
    "id": "6a7eec1f-873a-4c71-97f2-ee3492dd14f9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
