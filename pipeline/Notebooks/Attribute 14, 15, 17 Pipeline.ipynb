{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528b55a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Chen\n",
      "[nltk_data]     Wei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Chen\n",
      "[nltk_data]     Wei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Chen\n",
      "[nltk_data]     Wei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "######################################### IMPORTING PACAKGES #############################\n",
    "# Basic ML Packages\n",
    "from scipy import spatial\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# PDF text extraction\n",
    "from pdfminer3.layout import LAParams, LTTextBox\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "from pdfminer3.pdfinterp import PDFResourceManager\n",
    "from pdfminer3.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer3.converter import PDFPageAggregator\n",
    "from pdfminer3.converter import TextConverter\n",
    "\n",
    "# Others\n",
    "import string\n",
    "import re\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "import io\n",
    "\n",
    "# Text pre-processing (Tokenization, Stemming, Lemmatization)\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Pdf Extraction Model\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "#Gensim stopwords\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "stopwords = gensim.parsing.preprocessing.STOPWORDS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import tabula\n",
    "from tabula import read_pdf\n",
    "import io\n",
    "from functools import reduce\n",
    "from pdfminer.high_level import extract_text\n",
    "import pdf2image\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d887f72",
   "metadata": {},
   "source": [
    "## New Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e32f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    df = df.drop_duplicates()\n",
    "    bert=KeyBERT()\n",
    "    kw = []\n",
    "    for i in tqdm(df['words']):\n",
    "        kw.append(bert.extract_keywords(i, keyphrase_ngram_range=(2, 2), stop_words='english'))\n",
    "    df['kw'] = kw\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b86724cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_filter(df, keywords):\n",
    "    def func(kw, key):\n",
    "        if any(any(w in word[0] for w in key) for word in kw):\n",
    "            return True\n",
    "    \n",
    "    df_filtered = df[df['kw'].apply(lambda x: func(x, keywords)) == True]\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23e69be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(df, embed_column, attribute_no, embedding_model='tfidf'):\n",
    "    if embedding_model == 'tfidf': ##save fit model and transform here\n",
    "        X = df[embed_column]\n",
    "        X = X.apply(lambda x: x.lower())\n",
    "        if attribute_no == 14:\n",
    "            tfidf = pickle.load(open('models/tfidf_14_model.sav', 'rb'))\n",
    "        elif (attribute_no == 7) or (attribute_no == 15):\n",
    "            tfidf = pickle.load(open('models/tfidf_15_model.sav', 'rb'))\n",
    "        elif attribute_no == 17:\n",
    "            tfidf = pickle.load(open('models/tfidf_17_model.sav', 'rb'))\n",
    "        else:\n",
    "            raise Exception(f\"Wrong Model used for attribute: {attribute_no}\")\n",
    "        x = tfidf.transform(X)\n",
    "        X_encoded = pd.DataFrame(x.toarray())\n",
    "        return X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "368a03e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_filtering(df):\n",
    "    model_name = \"deepset/roberta-base-squad2\"\n",
    "    nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "    \n",
    "    res = []\n",
    "    q1 = 'Who audited the targets?'\n",
    "    q2 = 'Who assured the targets?'\n",
    "    q3 = 'Who verified the targets?'\n",
    "    for i in df['sentence']:\n",
    "        QA_1 = {\n",
    "            'question': q1,\n",
    "            'context': i\n",
    "        }\n",
    "        QA_2 = {\n",
    "            'question': q2,\n",
    "            'context': i\n",
    "        }\n",
    "        QA_3 = {\n",
    "            'question': q3,\n",
    "            'context': i\n",
    "        }\n",
    "\n",
    "        ans1 = nlp(QA_1)['answer']\n",
    "        score1 = nlp(QA_1)['score']\n",
    "        ans2 = nlp(QA_2)['answer']\n",
    "        score2 = nlp(QA_2)['score']\n",
    "        ans3 = nlp(QA_3)['answer']\n",
    "        score3 = nlp(QA_3)['score']\n",
    "\n",
    "        maxi = max([score1, score2, score3])\n",
    "        if maxi == score1:\n",
    "            res.append(ans1)\n",
    "        elif maxi == score2:\n",
    "            res.append(ans2)\n",
    "        else:\n",
    "            res.append(ans3)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b447af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_images(file_path,  output_path, keywords=[r'scope \\d', 'location-based', 'market-based'], table_only=True):\n",
    "    dataset_list = os.listdir(file_path)\n",
    "    for file in dataset_list:\n",
    "        #reading pdf file to filter keywords\n",
    "        pdfFile = open(file_path + '/' + file, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFile)\n",
    "        totpages = pdfReader.numPages\n",
    "        \n",
    "        print(\"Starting with file: \" + file)\n",
    "        page_with_keywords = []\n",
    "        for p in range(pdfReader.numPages):\n",
    "            text = pdfReader.pages[p].extract_text().lower()\n",
    "            if any(re.search(x, text) for x in keywords):\n",
    "                if (p+1) not in page_with_keywords:\n",
    "                    page_with_keywords.append(p + 1)\n",
    "        \n",
    "        ## Filter for only tables.\n",
    "        if table_only:\n",
    "            table_pages = []\n",
    "            for i in page_with_keywords:\n",
    "                pdf = read_pdf(file_path + '/' + file, pages=i, stream=True, pandas_options={'header':'None'}, multiple_tables=True)\n",
    "                if len(pdf) > 0:\n",
    "                    table_pages.append(i)\n",
    "            page_with_keywords = table_pages\n",
    "        \n",
    "        ##Extract images\n",
    "        for i in page_with_keywords:\n",
    "            pdf2image.convert_from_path(file_path + '/' + file, output_folder = output_path, fmt='png', \n",
    "                                       first_page = i, last_page = i, output_file = str(file) + str(i))\n",
    "        \n",
    "        print('Finished with file: ' + file)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7aacc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_7(df):\n",
    "    return attribute_15(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4fd13613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_14(df):\n",
    "    df = keyword_filter(df, ['ghg', 'sbti', 'tcfd', 'sasb', r'scope /d'])\n",
    "    df['preprocessed'] = df['sentence'].apply(lambda x: pre_processing(x))\n",
    "    if df.empty:\n",
    "        return df\n",
    "    X = word_embedding(df, 'preprocessed', 14)\n",
    "    lr_model = pickle.load(open('models/lr_14_model.sav', 'rb'))\n",
    "    rf_model = pickle.load(open('models/rf_14_model.sav', 'rb'))\n",
    "    svc_model = pickle.load(open('models/svc_14_model.pkl', 'rb'))\n",
    "    \n",
    "    lr_pred = lr_model.predict(X)\n",
    "    rf_pred = rf_model.predict(X)\n",
    "    svc_pred = svc_model.predict(X)\n",
    "   \n",
    "    ## Ensemble voting\n",
    "    df_combi = pd.DataFrame([lr_pred, rf_pred, svc_pred]).transpose()\n",
    "    df_combi['total'] = df_combi.mode(axis=1)[0]\n",
    "    df = df.reset_index()\n",
    "    df['flag'] = df_combi['total']\n",
    "    \n",
    "    ### return 1s only\n",
    "    df_ones = df[df['flag'] == 1]\n",
    "    \n",
    "    for index, rows in df_ones.iterrows():\n",
    "        res = []\n",
    "        if ('ghg' in rows['sentence'].lower()) or (r'scope \\d' in rows['sentence'].lower()):\n",
    "            res.append('GHG')\n",
    "        if ('sbti' in rows['sentence'].lower()) or ('science based targets' in rows['sentence'].lower()):\n",
    "            res.append('SBTi')\n",
    "        if ('tcfd' in rows['sentence'].lower()) or ('climate-related financial disclosures' in rows['sentence'].lower()):\n",
    "            res.append('TCFD')\n",
    "        if ('sasb' in rows['sentence'].lower()) or ('sustainability accounting' in rows['sentence'].lower()):\n",
    "            res.append('SASB')\n",
    "    \n",
    "        df_ones.at[index, 'methodologies'] = str(res)\n",
    "    df_ones = df_ones[['sentence', 'methodologies', 'flag']]\n",
    "    return df_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f47319fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_15(df, further_precision=True):\n",
    "    df = keyword_filter(df, ['assurance', 'limited assurance', 'externally verified', 'independent', 'third-party'])\n",
    "    df['preprocessed'] = df['sentence'].apply(lambda x: pre_processing(x))\n",
    "    if df.empty:\n",
    "        return df\n",
    "    X = word_embedding(df, 'preprocessed', 15)\n",
    "    \n",
    "    ada_model = pickle.load(open('models/ada_15_model.pkl', 'rb'))\n",
    "    svc = pickle.load(open('models/svc_15.pkl', 'rb'))\n",
    "    tfidf_2 = pickle.load(open('models/tfidf_15_2.pkl', 'rb'))\n",
    "    \n",
    "    ada_pred = ada_model.predict(X)\n",
    "    \n",
    "    ##return 1s only\n",
    "    df['flag'] = ada_pred\n",
    "    df_ones = df[df['flag'] == 1]\n",
    "    \n",
    "    if further_precision:\n",
    "        new_X = df_ones['preprocessed']\n",
    "        if new_X.size != 0:\n",
    "            x = tfidf_2.transform(new_X)\n",
    "            new_test_X = pd.DataFrame(x.toarray())\n",
    "            sv_pred = svc.predict(new_test_X)\n",
    "\n",
    "            df_ones['further_flag'] = sv_pred\n",
    "            df_verified = df_ones[df_ones['further_flag'] == 1]\n",
    "        else:\n",
    "            df_verified = pd.DataFrame()\n",
    "\n",
    "        if not df_verified.empty:\n",
    "            res = qa_filtering(df_verified)\n",
    "            df_verified['auditors'] = res\n",
    "            df_verified = df_verified[['sentence', 'auditors', 'further_flag']]\n",
    "            return df_verified\n",
    "        else:\n",
    "            print(\"Unable to conduct further separation. Original separation will be used instead.\")\n",
    "    \n",
    "    res = qa_filtering(df_ones)\n",
    "    df_ones['auditors'] = res\n",
    "\n",
    "    df_ones = df_ones[['sentence', 'auditors', 'flag']]\n",
    "    return df_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d9c100ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_17(df):\n",
    "    df = keyword_filter(df, ['compensation', 'remuneration'])\n",
    "    df['preprocessed'] = df['sentence'].apply(lambda x: pre_processing(x))\n",
    "    if df.empty:\n",
    "        return df\n",
    "    X = word_embedding(df, 'preprocessed', 17)\n",
    "    \n",
    "    lr_model = pickle.load(open('models/lr_17_model.sav', 'rb'))\n",
    "    ada_model = pickle.load(open('models/ada_17_model.sav', 'rb'))\n",
    "    \n",
    "    \n",
    "    ada_pred = ada_model.predict(X)\n",
    "    \n",
    "    df['flag'] = ada_pred\n",
    "    \n",
    "    ## Returns 1s only\n",
    "    df_ones = df[df['flag'] == 1]\n",
    "    \n",
    "    df_ones = df_ones[['sentence', 'flag']]\n",
    "    return df_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87670888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(line):\n",
    "    line = re.sub(r'[0-9\\.]+', '', line) # remove digits\n",
    "    line = re.sub(r'[^\\w\\s]','', line) # remove punctuation\n",
    "    return line\n",
    "\n",
    "def stemming(line):\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    return [stemmer.stem(token) for token in line]\n",
    "\n",
    "def lemmatization(line):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in line]\n",
    "\n",
    "def remove_stop_words(line):\n",
    "    return [remove_stopwords(token) for token in line]\n",
    "\n",
    "def pre_processing(line):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "    tokenized_line = tokenizer.tokenize(clean(line))\n",
    "    preprocessed_line = stemming(lemmatization(remove_stop_words(tokenized_line)))\n",
    "    \n",
    "    return ' '.join([token for token in preprocessed_line if token != ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9326c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_filter(df, keywords):\n",
    "    filtered = []\n",
    "    for s in np.array(df['sentence']):\n",
    "        sentence = s.lower()\n",
    "        for k in keywords:\n",
    "            if k in sentence:\n",
    "                filtered.append([s, k])\n",
    "    \n",
    "    filtered_df = pd.DataFrame(filtered, columns=['sentence', 'keyword(s)']).groupby(['sentence']).agg({'keyword(s)': lambda x: list(x.unique())}).reset_index()\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4148697",
   "metadata": {},
   "source": [
    "## Test PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4e850881",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_csv('citycon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6ccb6a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.rename(columns={'words': 'sentence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "778534a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ghg', 'sbti', 'tcfd', 'sasb', 'scope /d']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = ['ghg', 'sbti', 'tcfd', 'sasb', r'scope /d']\n",
    "keywords = [x.lower() for x in keywords]\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd641c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>keyword(s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Energy indicators real estate development Tota...</td>\n",
       "      <td>[ghg]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GRI Standard Disclosure Page Omission, Reason ...</td>\n",
       "      <td>[tcfd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRI Standard Disclosure Page Omission, Reason ...</td>\n",
       "      <td>[ghg, tcfd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 2021 UBM applied for membership of the UN G...</td>\n",
       "      <td>[tcfd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One related step was the announcement in 2021 ...</td>\n",
       "      <td>[tcfd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The TCFD recommendations on the reporting of c...</td>\n",
       "      <td>[tcfd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The TCFD recommends the voluntary disclosure o...</td>\n",
       "      <td>[tcfd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>There were a number of changes in comparison w...</td>\n",
       "      <td>[ghg]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UBM has also been an official supporter of the...</td>\n",
       "      <td>[tcfd]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence   keyword(s)\n",
       "0  Energy indicators real estate development Tota...        [ghg]\n",
       "1  GRI Standard Disclosure Page Omission, Reason ...       [tcfd]\n",
       "2  GRI Standard Disclosure Page Omission, Reason ...  [ghg, tcfd]\n",
       "3  In 2021 UBM applied for membership of the UN G...       [tcfd]\n",
       "4  One related step was the announcement in 2021 ...       [tcfd]\n",
       "5  The TCFD recommendations on the reporting of c...       [tcfd]\n",
       "6  The TCFD recommends the voluntary disclosure o...       [tcfd]\n",
       "7  There were a number of changes in comparison w...        [ghg]\n",
       "8  UBM has also been an official supporter of the...       [tcfd]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_filter(res, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61a2f745",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res_u' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mres_u\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: pre_processing(x))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'res_u' is not defined"
     ]
    }
   ],
   "source": [
    "res_u['sentence'].apply(lambda x: pre_processing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fbcb60bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with file: ubm_esg_report_2021.pdf\n",
      "Finished with file: ubm_esg_report_2021.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_images(DATA_FOLDER, 'test_dataset/images', table_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cefdffdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The dividend is paid in the following nancial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The dividend proposal for 2021 is subject to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G as in Governance UBM &amp; Sustainability 3.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Important information 7.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maly-GrtnerCOODear Shareholders,Dear Stakehold...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>The amounts were rounded based on the compensa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>However, rounding, typesetting and printing er...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>This ESG report is published in English and Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>In the event of a discrepancy or deviation, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>This ESG report was printed on Olin regular ab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1105 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence\n",
       "0     The dividend is paid in the following nancial ...\n",
       "1     The dividend proposal for 2021 is subject to t...\n",
       "2            G as in Governance UBM & Sustainability 3.\n",
       "3                              Important information 7.\n",
       "4     Maly-GrtnerCOODear Shareholders,Dear Stakehold...\n",
       "...                                                 ...\n",
       "1100  The amounts were rounded based on the compensa...\n",
       "1101  However, rounding, typesetting and printing er...\n",
       "1102  This ESG report is published in English and Ge...\n",
       "1103  In the event of a discrepancy or deviation, th...\n",
       "1104  This ESG report was printed on Olin regular ab...\n",
       "\n",
       "[1105 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c211b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "13dd59f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_14 = attribute_14(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7ab44e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>methodologies</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CARBON In calculating its carbon footprint, Citycon applies the Greenhouse Gas Protocol (GHG) developed by the World Resources Institute and the World Business Council for Sustaina-ble Development.</td>\n",
       "      <td>['GHG']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Following this, in October 2021 we became the first real estate company in Finland to join the Science Based Targets initiative (SBTi).</td>\n",
       "      <td>['SBTi']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joining SBTi is based on their validation.</td>\n",
       "      <td>['SBTi']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>We continue to excel on our climate goals: We are committed to the SBTi (Science Based Targets initiative) to reduce 50% of our scope 1 and 2 emissions by 2030 when compared to our emissions in 2018.</td>\n",
       "      <td>['SBTi']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                  sentence  \\\n",
       "1    CARBON In calculating its carbon footprint, Citycon applies the Greenhouse Gas Protocol (GHG) developed by the World Resources Institute and the World Business Council for Sustaina-ble Development.   \n",
       "2                                                                  Following this, in October 2021 we became the first real estate company in Finland to join the Science Based Targets initiative (SBTi).   \n",
       "3                                                                                                                                                               Joining SBTi is based on their validation.   \n",
       "6  We continue to excel on our climate goals: We are committed to the SBTi (Science Based Targets initiative) to reduce 50% of our scope 1 and 2 emissions by 2030 when compared to our emissions in 2018.   \n",
       "\n",
       "  methodologies  flag  \n",
       "1       ['GHG']     1  \n",
       "2      ['SBTi']     1  \n",
       "3      ['SBTi']     1  \n",
       "6      ['SBTi']     1  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d37d234a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to conduct further separation. Original separation will be used instead.\n"
     ]
    }
   ],
   "source": [
    "df_15 = attribute_15(res, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0cae7af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>auditors</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sentence, auditors, flag]\n",
       "Index: []"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "df2a2bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_17 = attribute_17(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f7b86ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>keyword(s)</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sentence, keyword(s), preprocessed]\n",
       "Index: []"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_17"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
